{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Skills + Ollama (Local LLMs)\n",
    "\n",
    "This notebook demonstrates how to use AI Skills with locally-running Ollama models.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Install Ollama: https://ollama.ai\n",
    "2. Pull a model: `ollama pull llama3.1`\n",
    "3. Ensure Ollama is running: `ollama serve`\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aiskills[ollama,search] -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Ollama Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiskills.integrations import create_ollama_client\n",
    "\n",
    "client = create_ollama_client(model=\"llama3.1\")\n",
    "\n",
    "# Check if model is available\n",
    "if client.is_model_available():\n",
    "    print(\"llama3.1 is available!\")\n",
    "else:\n",
    "    print(\"llama3.1 not found. Run: ollama pull llama3.1\")\n",
    "\n",
    "# List available models\n",
    "print(\"\\nAvailable models:\")\n",
    "for model in client.list_local_models():\n",
    "    print(f\"  - {model['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start: Tool Calling (Recommended)\n",
    "\n",
    "For models that support tool calling (llama3.1, mistral, qwen2, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiskills.integrations import create_ollama_client\n",
    "\n",
    "# Tool calling is auto-enabled for supported models\n",
    "client = create_ollama_client(model=\"llama3.1\")\n",
    "\n",
    "response = client.chat(\"Help me debug a memory leak in Python\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Injection Mode\n",
    "\n",
    "For models without tool support (codellama, phi, etc.), use prompt injection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tools, use prompt injection instead\n",
    "client = create_ollama_client(model=\"llama3.1\", use_tools=False)\n",
    "\n",
    "# Skill content is injected into the prompt\n",
    "response = client.chat_with_skill(\n",
    "    skill_query=\"python debugging\",\n",
    "    user_message=\"How do I find where my memory is being leaked?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Skill Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available skills\n",
    "skills = client.list_skills()\n",
    "print(f\"Found {len(skills)} skills:\\n\")\n",
    "for skill in skills[:5]:\n",
    "    print(f\"  - {skill['name']}: {skill['description'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for skills\n",
    "results = client.search_skills(\"testing\")\n",
    "print(f\"Found {results.total} testing-related skills:\")\n",
    "for r in results.results:\n",
    "    print(f\"  - {r['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a skill directly\n",
    "result = client.use_skill(\"write unit tests for Python\")\n",
    "if result.success:\n",
    "    print(f\"Skill: {result.skill_name}\")\n",
    "    print(f\"Tokens: {result.tokens_used}\")\n",
    "    print(f\"\\nPreview:\\n{result.content[:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Mode\n",
    "\n",
    "For code completions instead of chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = create_ollama_client(model=\"llama3.1\", use_tools=False)\n",
    "\n",
    "# Generate code with skill context\n",
    "code = client.generate_with_skill(\n",
    "    skill_query=\"python testing\",\n",
    "    prompt=\"Write pytest tests for this function:\\n\\ndef add(a, b):\\n    return a + b\"\n",
    ")\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Models\n",
    "\n",
    "Compare responses from different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_try = [\"llama3.1\", \"mistral\", \"qwen2\"]\n",
    "question = \"What's the best way to handle errors in Python?\"\n",
    "\n",
    "for model_name in models_to_try:\n",
    "    try:\n",
    "        client = create_ollama_client(model=model_name)\n",
    "        if client.is_model_available():\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print('='*50)\n",
    "            response = client.chat(question)\n",
    "            print(response[:500] + \"...\" if len(response) > 500 else response)\n",
    "        else:\n",
    "            print(f\"\\n{model_name}: Not installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{model_name}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Calling Details\n",
    "\n",
    "See exactly what tools are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiskills.integrations import get_ollama_tools\n",
    "import json\n",
    "\n",
    "tools = get_ollama_tools()\n",
    "print(\"Available tools for Ollama:\\n\")\n",
    "for tool in tools:\n",
    "    func = tool['function']\n",
    "    print(f\"Tool: {func['name']}\")\n",
    "    print(f\"  Description: {func['description'][:80]}...\")\n",
    "    print(f\"  Parameters: {list(func['parameters']['properties'].keys())}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Chat Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiskills.integrations import create_ollama_client\n",
    "\n",
    "client = create_ollama_client(model=\"llama3.1\")\n",
    "\n",
    "print(\"Chat with Ollama + AI Skills (type 'quit' to exit)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \")\n",
    "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "        break\n",
    "    \n",
    "    response = client.chat(user_input)\n",
    "    print(f\"\\nAssistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tips\n",
    "\n",
    "1. **Use smaller models** for faster responses: `gemma2:2b`, `phi3:mini`\n",
    "2. **Prompt injection** is faster than tool calling for simple queries\n",
    "3. **Pre-load skills** if you know what you need\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **Connection refused**: Ensure `ollama serve` is running\n",
    "- **Model not found**: Run `ollama pull <model>`\n",
    "- **Slow responses**: Try a smaller model or use `chat_with_skill()` for simpler flows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
